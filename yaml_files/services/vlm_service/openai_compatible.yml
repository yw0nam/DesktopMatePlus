# OpenAI Compatible VLM Service Configuration
# API credentials are loaded from environment variables (.env file):
#   - VLM_API_KEY (optional, defaults to "token-abc123" for local servers)
#   - VLM_BASE_URL (required, defaults to "http://localhost:8001")
#   - VLM_MODEL_NAME (required, defaults to "chat_model")

vlm_config:
  type: "openai_chat_agent"  # Service type for factory
  configs:
    temperature: 0.7
    top_p: 0.9
    model_name: "chat_model"
    openai_api_base: "http://localhost:5530/v1"
    # openai_api_key: "token-abc123"  # Loaded from VLM_API_KEY environment variable
