# Agent Service Configuration
# OpenAI-compatible LLM with mem0 memory and PostgreSQL vocabulary manager

# Agent Service Configuration
llm_config:
  type: "openai_chat_agent"  # Service type for factory
  configs:
    openai_api_base: "http://localhost:55235/v1"
    model_name: chat_model
    temperature: 0.7
    top_p: 0.9
    support_image: True  # Enable image support
    # openai_api_key is loaded from LLM_API_KEY environment variable

# mcp_config:
#   "sequential-thinking": {
#       "command": "npx",
#       "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"],
#       "transport": "stdio",
#   }

# Note: configurations of memory and vocabulary manager should be set in .env files
