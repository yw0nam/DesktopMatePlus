# Agent Service Configuration
# OpenAI-compatible LLM with mem0 memory and PostgreSQL vocabulary manager

# Agent Service Configuration
llm_config:
  type: "openai_chat_agent"  # Service type for factory
  configs:
    openai_api_base: "http://localhost:5580/v1"
    model: "chat_model"
    # openai_api_key is loaded from LLM_API_KEY environment variable
    temperature: 0.7
    top_p: 0.9

mcp_config:
  "sequential-thinking": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"],
      "transport": "stdio",
  }

# Note: configurations of memory and vocabulary manager should be set in .env files
