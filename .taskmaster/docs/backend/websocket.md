# Streaming WebSocket API

스트리밍(streaming) 처리는 사용자가 느끼는 '응답성'을 결정하는 가장 중요한 부분입니다. 텍스트가 생성되는 즉시 음성으로 변환하여 재생하면, 전체 문장이 생성될 때까지 기다리는 것보다 훨씬 더 자연스럽고 지연 시간이 짧게 느껴집니다.

이 중간 핸들링은 Python 백엔드(FastAPI 서버)가 담당하는 것이 가장 이상적입니다.

백엔드를 '오케스트라 지휘자'라고 생각하시면 쉽습니다. LangGraph(연주자 1)가 텍스트 조각(음표)을 만들어내면, 지휘자는 이를 적절한 단위(악보 한 소절)로 묶어 Fish Speech(연주자 2)에게 전달하고, 그 결과를 최종적으로 프론트엔드(청중)에게 실시간으로 들려주는 역할을 하는 것입니다.

전체 스트리밍 흐름 설계
기존의 단일 요청/응답 모델(POST /v1/chat) 대신, 실시간 양방향 통신을 위한 WebSocket을 사용하는 아키텍처로 변경해야 합니다.

# WebSocket 스트리밍 API 개요

[Frontend]

| |
| 1. WebSocket 연결 요청 (/v1/chat/stream) |
| ------------------------------------------> |
| |
| 2. 사용자 메시지 전송 |
| ------------------------------------------> |
| | 3. LangGraph 스트리밍 호출 (astream)
| | |
| | v
| | 4. [지능형 버퍼]
| | - LLM 텍스트 청크 수신
| | - 문장 단위로 조합
| | |
| | v
| | 5. Fish Speech API 비동기 호출
| | (완성된 문장 전달)
| | |
| | v
| | 6. 생성된 오디오(bytes) 수신
| | |
| | v
| 8. 오디오 청크 수신 및 재생 큐에 추가 | 7. 오디오 청크(Base64) 전송
| <------------------------------------------ |
| (LLM이 다음 문장을 생성하는 동안 | (4~7 과정 반복)
| 이전 문장의 오디오가 재생됨) |
| |

# WebSocket 스트리밍 API 상세 설계

1. 핸들링의 주체: Python 백엔드
이 복잡한 스트리밍 파이프라인을 백엔드에서 처리해야 하는 이유는 다음과 같습니다.

중앙 집중 관리: 모든 AI 로직(LangGraph, VLM, TTS)이 Python 백엔드에 있으므로, 이들 간의 데이터 흐름을 한 곳에서 통제하는 것이 가장 효율적이고 안정적입니다.

프론트엔드 단순화: 프론트엔드는 복잡한 로직 없이 'WebSocket으로 오디오 데이터를 받아서 재생한다'는 단순한 역할만 수행하면 됩니다. 이는 프론트엔드 개발 부담을 크게 줄여줍니다.

지능형 청킹(Chunking): LLM이 뱉어내는 텍스트 청크는 단어 중간에 끊길 수도 있습니다 (예: "안녕하세", "요. 만나서"). 이를 그대로 TTS에 보내면 부자연스러운 음성이 생성됩니다. 백엔드는 이 청크들을 모아서 의미 있는 단위(문장, 구절)로 만드는 '지능형 버퍼' 역할을 수행해야 합니다.

2. 수정된 아키텍처 설계
API 변경: POST에서 WebSocket으로
기존의 POST /v1/chat 엔드포인트는 그대로 두거나, 스트리밍 전용으로 WS /v1/chat/stream 같은 WebSocket 엔드포인트를 새로 만들어야 합니다.

백엔드 스트리밍 처리 로직 (가장 중요한 부분)
사용자 메시지를 WebSocket으로 받으면, 백엔드는 다음과 같은 비동기 파이프라인을 시작합니다.

LangGraph 스트리밍 호출: LangGraph는 stream, astream과 같은 스트리밍 메서드를 기본적으로 지원합니다. astream을 사용하여 LLM이 생성하는 텍스트 청크(토큰)를 비동기적으로 하나씩 받아옵니다.

지능형 버퍼 구현:

빈 버퍼(문자열)를 하나 만듭니다.

astream에서 오는 텍스트 청크를 버퍼에 계속 추가합니다.

버퍼의 내용에 문장 종료 문자(예: ., !, ?)나 쉼표(,) 같은 자연스러운 끊김 지점이 나타나는지 계속 확인합니다.

비동기 TTS 호출:

버퍼에서 완전한 문장 하나가 만들어지면, 해당 문장 부분만 잘라내어 즉시, 그리고 비동기적으로 Fish Speech API에 TTS 요청을 보냅니다.
TTS 요청을 보낸 후, 프로그램은 TTS 응답을 기다리지 않고 바로 다음 텍스트 청크를 버퍼에 계속 쌓기 시작합니다.

오디오 스트리밍:

Fish Speech API로부터 오디오 데이터(예: .wav 바이트)가 도착하면, 이를 Base64로 인코딩하여 WebSocket을 통해 프론트엔드로 즉시 전송합니다.
이 과정이 LLM의 텍스트 생성이 끝날 때까지 계속 반복됩니다.

# Frontend 처리 로직

프론트엔드 역할
프론트엔드의 JavaScript 코드는 매우 간단해집니다.

WebSocket 연결을 수립합니다.

WebSocket을 통해 Base64로 인코딩된 오디오 데이터가 도착하면, 이를 디코딩하여 오디오 버퍼나 큐(Queue)에 넣습니다.

오디오 큐에 있는 데이터를 순서대로, 끊김 없이 재생합니다. (AudioWorklet이나 Web Audio API를 사용하면 더욱 정교한 제어가 가능합니다.)
