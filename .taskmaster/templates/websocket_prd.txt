
<PRD\>

# Overview

This document outlines the requirements for a real-time conversational agent backend.

The primary problem this system solves is the high perceived latency in traditional request-response chatbot interactions. Users typically have to wait until the AI has generated its entire response before they see or hear anything.

This system is designed for developers building highly interactive applications (such as desktop assistants, voice interfaces, or advanced chatbots) that require immediate, human-like feedback.

Its value lies in dramatically reducing the time-to-first-response by simultaneously streaming data for two separate purposes: a real-time "typing effect" for the UI and complete, cleaned sentence chunks for a Text-to-Speech (TTS) engine. This creates a much more engaging and natural user experience.

# Core Features

    * **Sentence Streaming for Clients**

            * **What it does:** Emits cleaned, sentence-level chunks to clients while keeping raw token flow internal to the server.
            * **Why it's important:** Provides a single, stable contract for UI and voice playback while allowing the backend to evolve its internal buffering strategy without breaking clients.
            * **How it works:** LangGraph tokens are placed on an internal queue, processed by `TextChunkProcessor` to identify sentence boundaries, cleaned by `TTSTextProcessor`, and finally emitted to clients as `tts_ready_chunk` events. No `stream_token` payloads are exposed outside the server.

  * **Real-time Agent State Transparency**

      * **What it does:** Notifies the client in real-time when the agent begins and ends internal tasks, such as calling a tool.
      * **Why it's important:** Provides the end-user with immediate feedback on what the AI is "thinking" or "doing" (e.g., "Searching for weather..."), reducing uncertainty and improving the perceived responsiveness during tool-use delays.
      * **How it works:** The system intercepts `on_tool_start` and `on_tool_end` events from the LangGraph stream and forwards them to the client as dedicated `tool_call_start` and `tool_call_end` WebSocket events.

  * **Interruptible Conversation Flow**

      * **What it does:** Allows the client to send an `interrupt_stream` message to immediately and safely stop the agent's current response generation.
      * **Why it's important:** Gives the user full control over the conversation, emulating the natural ability to "cut off" or redirect a speaker.
      * **How it works:** A central `MessageProcessor` class manages all `asyncio.Task`s related to a single conversational turn. Receiving an `interrupt_stream` message triggers a cancellation command on these tasks, safely tearing down the entire streaming pipeline for that turn.

  * **Persistent Conversational Memory**

      * **What it does:** Remembers the context and history of a conversation across multiple interactions and sessions.
      * **Why it's important:** Enables stateful, multi-turn dialogues where the agent can refer to previous messages.
      * **How it works:** The system uses a LangGraph Checkpointer (e.g., `RedisSaver` in production) linked to a `conversation_id`. This ID is used to load the conversation's history before executing a new turn and save the new messages upon completion.

# User Experience

  * **User Personas**

      * **Developer (Primary User):** An application developer who needs a backend API to power a conversational interface. They will consume the WebSocket API defined in this document.
      * **End-User (Secondary User):** The individual interacting with the final application (e.g., a desktop assistant). They experience the system's output as real-time text and audio.

  * **Key User Flows**

    1.  **Connection:** The client application establishes a WebSocket connection and sends an `authorize` message. The server responds with `authorize_success`.
    2.  **User Inquiry:** The end-user provides input (text or speech). The client sends a `send_message` event to the server, including the `conversation_id` and user input.
    3.  **AI Response & Streaming:**
        * Server sends `stream_start`.
        * Server streams `tts_ready_chunk` events (e.g., "Hello, how can I help you today?") as soon as complete sentences are available. The client can render text immediately and queue the same chunk for TTS playback.
        * (If a tool is used) Server sends `tool_call_start` (UI shows "Searching..."). After completion, it sends `tool_call_end`.
        * Server sends `stream_end` once the full response is generated.
    4.  **User Interruption:** The end-user speaks or clicks "stop." The client sends an `interrupt_stream` message. The server immediately stops sending `tts_ready_chunk` events for that turn.
    5.  **Disconnection:** The client disconnects. The server cleans up all associated connection-level resources.

  * **UI/UX Considerations**

    * The client UI consumes a single stream of `tts_ready_chunk` events. If a typing animation is desired, the UI should derive it from the latest chunk, since raw token flow remains internal.
    * The client should implement a visual indicator (e.g., a "thinking" spinner) that activates upon receiving `tool_call_start` and deactivates on `tool_call_end`.
    * The client-side TTS playback mechanism must queue incoming `tts_ready_chunk`s to ensure smooth, uninterrupted audio playback.

# Technical Architecture

  * **System Components**

      * **WebSocket Gateway:** The stateful entry point for all client communication. Manages connection lifecycles, authentication, message deserialization, and routing.
      * **Orchestration Core (MessageProcessor):** A class-based component instantiated for each conversational turn. It acts as a supervisor, managing the entire lifecycle of the LangGraph stream, the text processing pipeline, and all related `asyncio.Task`s to prevent resource leaks.
      * **LangGraph State Machine:** The "brain" of the agent, defined using `StateGraph`. It includes nodes for the LLM (`agent`) and `tool_executor`, with conditional edges to manage the conversation logic.
      * **Text Processing Pipeline:** A two-stage, stateful pipeline:
        1.  `TextChunkProcessor`: A stateful object that buffers tokens and yields complete sentences based on terminators.
        2.  `TTSTextProcessor`: A stateless, rule-based object that cleans the output chunks, removing artifacts (e.g., `(laughter)`) and expanding abbreviations to make them TTS-friendly.

  * **Data Models (State Management)**

      * **Connection State (Ephemeral):** Managed by the WebSocket Gateway. Includes `connection_id`, authentication status. Lasts only as long as the WebSocket connection.
      * **Conversation State (Persistent):** Managed by the LangGraph Checkpointer (e.g., `RedisSaver`). Includes the full message history, tied to a `conversation_id`. This state is durable and survives server restarts.
      * **Turn State (Ephemeral):** Managed by a `MessageProcessor` instance. Includes active `asyncio.Task` references and the internal buffer of the `TextChunkProcessor`. It is created on `send_message` and destroyed on `stream_end` or `interrupt`.

  * **APIs and Integrations**

      * **WebSocket API:** A single-endpoint (`wss://api.example.com/v1/chat/stream`) protocol for all real-time communication. The full message contract is defined in the Appendix (Table 1).
          * **Client-to-Server:** `authorize`, `send_message`, `interrupt_stream`, `pong`.
          * **Server-to-Client:** `authorize_success`, `stream_start`, `tts_ready_chunk`, `tool_call_start`, `tool_call_end`, `stream_end`, `error`, `ping`.
      * **LangGraph Integration:**
          * The system uses `graph.astream_events` to consume a fine-grained event stream.
          * The `MessageProcessor` listens specifically for `on_chat_model_stream`, `on_tool_start`, and `on_tool_end` events.
          * Persistent state is linked by passing `{"configurable": {"thread_id": conversation_id}}` to the graph's `astream_events` method.

  * **Infrastructure Requirements**

      * **Asynchronous Web Server:** A Python server capable of handling a large number of concurrent WebSocket connections (e.g., FastAPI, Quart).
      * **Persistent Checkpointer:** A Redis instance is required for the production-level `RedisSaver` to store conversation state.
      * **Observability Stack (Recommended):** A Prometheus/Grafana stack for monitoring key metrics (defined in 6.2 of the source PRD) and a structured logging aggregator (e.g., ELK stack).
      * **Backpressure Mechanism:** The `asyncio.Queue` used between the stream producer (LangGraph) and consumer (WebSocket) *must* be initialized with a `maxsize` to prevent memory exhaustion if the client consumes data slower than the server produces it.

# Development Roadmap

  * **MVP (Phase 1): Core Streaming Functionality**

    1.  **WebSocket Gateway:** Implement the connection, `authorize`, and `ping`/`pong` lifecycle.
    2.  **Basic Graph:** Define the `StateGraph` with an `agent` node and an in-memory `MemorySaver`.
    3.  **Core Orchestrator:** Build the `MessageProcessor` class to handle `send_message`.
    4.  **Producer/Consumer Loop:** Implement the `astream_events` producer and a consumer using an `asyncio.Queue(maxsize=...)`.
    5.  **Sentence Streaming:**
        * Build `TextChunkProcessor` (with `process` and `flush` methods).
        * Build a *basic* `TTSTextProcessor` (with hard-coded rules).
        * Emit `tts_ready_chunk` events as the sole client-visible text payload.
    6.  **Interrupts:** Implement the `interrupt_stream` message to cancel the active tasks in the `MessageProcessor`.
    7.  **Lifecycle Events:** Implement `stream_start` and `stream_end` events.
    9.  **Basic Error Handling:** Add a `try...except` block around the stream processing to send a generic `error` event on failure.

  * **Future Enhancements (Phase 2): Production Readiness**

    1.  **Persistent Memory:** Replace `MemorySaver` with `RedisSaver` and integrate it with the `conversation_id` provided by the client.
    2.  **Agent Transparency:** Add event handling for `on_tool_start`/`on_tool_end` to emit the `tool_call_start` and `tool_call_end` events.
    3.  **Configurable TTS:** Refactor `TTSTextProcessor` to load its cleaning rules from an external configuration file (e.g., YAML or JSON).
    4.  **Structured Logging:** Implement structured JSON logging across all components, ensuring `conversation_id` and `turn_id` are included in all logs for traceability.
    5.  **Observability:** Integrate Prometheus metrics for key indicators:
          * `websocket_active_connections`
          * `turn_processing_latency_seconds`
          * `time_to_first_token_seconds`
          * `langgraph_node_errors_total`

# Logical Dependency Chain

1.  **Foundation (Gateway):** Build the WebSocket server and connection management (`authorize`, `ping`/`pong`). This is the entry point.
2.  **Orchestration (MVP):** Build the `MessageProcessor` class to handle `send_message`, `interrupt`, and `cleanup`.
3.  **Brain (MVP):** Define the basic LangGraph with `MemorySaver`.
4.  **Visible Front-End Streaming:** Connect `MessageProcessor` to `graph.astream_events`, wire the internal token queue, and emit `tts_ready_chunk` events to clients.
5.  **UX Improvement (Transparency):** Add logic to handle `on_tool_start`/`on_tool_end` and emit the corresponding `tool_call_start`/`tool_call_end` events.
6.  **Productionizing (Persistence):** Swap the `MemorySaver` for `RedisSaver` to enable persistent, multi-session conversations.
7.  **Productionizing (Ops):** Layer in structured logging and Prometheus metrics for monitoring and debugging.
8.  **Refinement:** Externalize the `TTSTextProcessor` rules into a config file for easier maintenance.

# Risks and Mitigations

  * **Risk:** Asynchronous Task Leakage

      * **Description:** An `asyncio.Task` (like the LangGraph stream) continues running in the background after a client disconnects or an error occurs, leading to a resource leak.
      * **Mitigation:** The `MessageProcessor` class acts as a strict supervisor. It must maintain references to all tasks it spawns and implement a `cleanup()` method (called in a `finally` block or on disconnect) that explicitly cancels every active task.

  * **Risk:** Memory Exhaustion (Backpressure Failure)

      * **Description:** The LangGraph stream produces tokens much faster than the client/network can consume them, causing the intermediate `asyncio.Queue` to grow indefinitely and consume all server memory.
      * **Mitigation:** The `asyncio.Queue` *must* be initialized with a `maxsize` (e.g., `maxsize=100`). This creates natural backpressure, as the `await queue.put()` call will block, pausing the LangGraph producer until the consumer has space.

  * **Risk:** TTS Stream Contamination

      * **Description:** Text from different sources (e.g., a tool's output and the LLM's subsequent commentary) are concatenated in the `TextChunkProcessor` buffer, creating a nonsensical sentence for TTS.
      * **Mitigation:** The `MessageProcessor` must be responsible for the `TextChunkProcessor`'s lifecycle. It should create a *new* instance of `TextChunkProcessor` for each distinct semantic block of text (e.g., on `on_chat_model_start`) and call `flush()` on it at the end (e.g., on `on_chat_model_end`).

  * **Risk:** Lost Text (Incomplete Final Sentence)

      * **Description:** The final sentence of a response (e.g., "Thank you") may not have punctuation, causing it to remain in the `TextChunkProcessor`'s buffer and never be sent to the user.
      * **Mitigation:** The `TextChunkProcessor` must have a `flush()` method that returns any remaining text in its buffer. The `MessageProcessor` is responsible for calling `flush()` when the stream ends (`stream_end`).

  * **Risk:** Poor Quality Audio

      * **Description:** The TTS engine receives text containing artifacts (e.g., `(chuckles)`, `*sighs*`) or unexpanded abbreviations, resulting in unnatural audio.
      * **Mitigation:** The `TTSTextProcessor` must be implemented with a robust, configurable set of regex rules to clean and normalize the text chunks *before* they are sent to the client.

# Appendix

### **Table 1: WebSocket Message Schema and Event Types**

| Event Type | Direction | Description | Data Schema | Example |
| :--- | :--- | :--- | :--- | :--- |
| `authorize` | C2S | Authenticates the connection. | `{ "token": "..." }` | `{"type": "authorize", "payload": {"token": "..."}}` |
| `send_message` | C2S | Requests a new AI response stream. | `{ "conversation_id": "...", "input": "..." }` | `{"type": "send_message", "payload": {"conversation_id": "...", "input": "Hi"}}` |
| `interrupt_stream` | C2S | Requests immediate stop of the current stream. | `{ "conversation_id": "..." }` | `{"type": "interrupt_stream", "payload": {"conversation_id": "..."}}` |
| `pong` | C2S | Response to the server's `ping`. | `{}` | `{"type": "pong", "payload": {}}` |
| `authorize_success` | S2C | Confirms successful authentication. | `{}` | `{"event": "authorize_success", "data": {}}` |
| `stream_start` | S2C | Signals the start of a new AI response stream. | `{ "turn_id": "uuid" }` | `{"event": "stream_start", "data": {"turn_id": "abc-123"}}` |
| `tts_ready_chunk` | S2C | A complete, cleaned sentence chunk ready for TTS. | `{ "chunk": "string" }` | `{"event": "tts_ready_chunk", "data": {"chunk": "Hello, how can I help you?"}}` |
| `tool_call_start` | S2C | Informs the client that a tool call has started. | `{ "tool_name": "string", "tool_input": "object" }` | `{"event": "tool_call_start", "data": {"tool_name": "get_weather", "tool_input": {"city": "Seoul"}}}` |
| `tool_call_end` | S2C | Informs the client that a tool call has finished. | `{ "tool_name": "string", "tool_output": "string" }` | `{"event": "tool_call_end", "data": {"tool_name": "get_weather", "tool_output": "It is sunny."}}` |
| `stream_end` | S2C | Signals the end of the current AI response stream. | `{ "turn_id": "uuid" }` | `{"event": "stream_end", "data": {"turn_id": "abc-123"}}` |
| `error` | S2C | Transmits a server-side error to the client. | `{ "code": "int", "message": "string" }` | `{"event": "error", "data": {"code": 5001, "message": "Tool execution failed."}}` |
| `ping` | S2C | Server's heartbeat signal. | `{}` | `{"event": "ping", "data": {}}` |
</PRD>
