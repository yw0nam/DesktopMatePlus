{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "WebSocket Gateway Setup",
        "description": "Create the asynchronous WebSocket server (FastAPI) with connection lifecycle, authentication (`authorize`), heartbeat (`ping`/`pong`), and basic error handling.",
        "details": "• Define a single endpoint `wss://api.example.com/v1/chat/stream`.\n• On connection, generate a `connection_id` and store auth state in an in‑memory dict.\n• Implement message router that deserializes JSON, validates schema (using pydantic models), and forwards to the appropriate handler.\n• Send `authorize_success` on valid token, close with error on failure.\n• Implement periodic `ping` messages and expect `pong` replies.\n• Ensure graceful shutdown of connections and cleanup of per‑connection resources.",
        "testStrategy": "Use pytest‑asyncio and websockets client to: (a) connect and receive `ping`, reply `pong`; (b) send valid/invalid `authorize` payloads and verify `authorize_success` or error; (c) disconnect and assert server releases resources (no lingering tasks).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "MessageProcessor Core Orchestrator",
        "description": "Implement the `MessageProcessor` class that supervises a single conversational turn, tracks asyncio.Tasks, and provides cleanup and interruption logic.",
        "details": "• Define `MessageProcessor` with methods `start_turn(conversation_id, user_input)`, `handle_interrupt()`, and `cleanup()`.\n• Store a dict of active tasks (`self._tasks`) and a reference to the per‑turn `asyncio.Queue`.\n• Expose callbacks for `on_chat_model_stream`, `on_tool_start`, `on_tool_end` that forward events to the queue.\n• Ensure `cleanup()` cancels all tasks, empties the queue, and logs the turn end.\n• Integrate with the WebSocket router from Task 1: on `send_message` instantiate a new `MessageProcessor` and store it keyed by `conversation_id`.\n• On `interrupt_stream` call `handle_interrupt()`.\n• Use `asyncio.create_task` for each background coroutine.",
        "testStrategy": "Unit test `MessageProcessor` by mocking a token producer coroutine: verify tasks are registered, `cleanup()` cancels them, and `handle_interrupt()` stops token emission. Use `asyncio.wait_for` to ensure no dangling tasks after interruption.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Basic LangGraph StateGraph with MemorySaver",
        "description": "Define a minimal LangGraph `StateGraph` containing an LLM agent node and an in‑memory `MemorySaver` for turn‑based state persistence.",
        "details": "• Create a `StateGraph` with nodes: `agent` (LLM) and `tool_executor` (placeholder).\n• Use `MemorySaver` for the checkpointer during MVP.\n• Provide a helper `get_graph(conversation_id)` that returns `graph.astream_events(configurable={\"thread_id\": conversation_id})`.\n• Ensure the graph yields events `on_chat_model_stream`, `on_tool_start`, `on_tool_end`.\n• Add simple conditional edges to allow tool execution flow (can be a no‑op for MVP).",
        "testStrategy": "Integration test that creates a graph, runs `astream_events` with a short prompt, and asserts that token events are emitted and that the in‑memory history contains both user and assistant messages.",
        "priority": "medium",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Producer‑Consumer Pipeline with Backpressure",
        "description": "Implement the async producer that streams LangGraph events into a bounded `asyncio.Queue` and a consumer that reads from the queue for further processing.",
        "details": "• In `MessageProcessor.start_turn`, create `self._queue = asyncio.Queue(maxsize=100)`.\n• Launch a producer task: `asyncio.create_task(self._produce_events(graph))` where `_produce_events` iterates over `graph.astream_events` and `await self._queue.put(event)`.\n• The producer must handle `asyncio.CancelledError` to exit cleanly on interruption.\n• Consumer tasks will be added in later steps (UI stream, TTS stream, tool events).\n• Log queue size periodically for observability.\n• Ensure backpressure: when queue is full, `await self._queue.put` blocks, throttling the LangGraph stream.",
        "testStrategy": "Stress test by mocking a fast‑producing LangGraph that emits >200 events per second. Verify that the producer blocks when the queue reaches `maxsize` and that no `MemoryError` occurs. Use `asyncio.wait_for` to ensure the test completes within a reasonable time.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "UI Stream (Path A) – Token Emission",
        "description": "Consume token events from the queue and emit `stream_start`, `stream_token`, and `stream_end` WebSocket messages for the typing effect.",
        "details": "• Add a consumer coroutine `self._ui_stream()` started as a task in `MessageProcessor.start_turn`.\n• On first token event, send `stream_start` with a generated `turn_id` (UUID).\n• For each `on_chat_model_stream` token, send `stream_token` with payload `{ \"token\": token }`.\n• When the LangGraph stream finishes, send `stream_end` with the same `turn_id`.\n• Ensure ordering: UI stream must not emit `stream_end` before all queued tokens are sent.\n• Use the WebSocket connection object from Task 1 (`WebSocket` instance) to `await websocket.send_json(event)`.\n• Handle `WebSocketDisconnect` by invoking `MessageProcessor.cleanup()`.",
        "testStrategy": "Functional test using a real WebSocket client: send `send_message`, capture the sequence of events, and assert that `stream_start` appears first, followed by a series of `stream_token` events, and finally `stream_end`. Verify that the `turn_id` is consistent across start/end.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Interrupt Stream Handling",
        "description": "Allow the client to cancel an active turn via `interrupt_stream` and ensure all related tasks stop safely.",
        "details": "• In the WebSocket router, map `interrupt_stream` to `MessageProcessor.handle_interrupt()`.\n• `handle_interrupt` must:\n   1. Cancel the producer task and all consumer tasks (`self._tasks.values()`).\n   2. Drain the queue (`while not self._queue.empty(): self._queue.get_nowait()`).\n   3. Send a `stream_end` event with the current `turn_id` to guarantee client state consistency.\n   4. Call `cleanup()` to release resources.\n• Guard against race conditions where a token may be in flight; use `asyncio.wait` with timeout to ensure cancellation completes.\n• Log interruption reason for observability.",
        "testStrategy": "Integration test: start a long‑running turn (e.g., a prompt that yields many tokens), send `interrupt_stream` after a few tokens, and assert that no further `stream_token` or `tts_ready_chunk` events are received. Verify that a single `stream_end` is still emitted.",
        "priority": "high",
        "dependencies": [
          2,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "TextChunkProcessor & TTSTextProcessor Implementation",
        "description": "Create the two‑stage pipeline that buffers raw tokens, extracts complete sentences, and cleans them for TTS.",
        "details": "• `TextChunkProcessor`\n   - Internal buffer `self._buf` (string).\n   - `process(token: str) -> Optional[str]` appends token, checks for sentence terminators (`.?!` followed by whitespace or end‑of‑string). When a terminator is found, returns the completed sentence and clears the buffer up to that point.\n   - `flush() -> str` returns any remaining buffered text.\n• `TTSTextProcessor`\n   - Stateless class with method `clean(text: str) -> str` applying regex rules:\n        * Remove parenthetical annotations like `(laughter)`, `*sigh*`.\n        * Expand common abbreviations (e.g., `i.e.` → `that is`).\n        * Normalize whitespace.\n   - Load optional rule set from a JSON/YAML file (future‑proof, but provide a default hard‑coded dict for MVP).\n• Include unit tests for edge cases: multiple terminators, missing punctuation, and cleaning of artifacts.\n• Ensure both classes are thread‑safe (no shared mutable state).",
        "testStrategy": "Unit tests covering:\n  - Buffering of tokens into sentences (e.g., tokens `['Hello', ',', 'world', '!']` → `\"Hello, world!\"`).\n  - `flush()` returns leftover text when no terminator.\n  - Cleaning rules remove `(laugh)` and expand `e.g.` correctly.\n  - Integration test feeding a stream of tokens through both processors and verifying the final TTS chunks match expected cleaned sentences.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "TTS Stream (Path B) – Chunk Emission",
        "description": "Consume token events, run them through `TextChunkProcessor` and `TTSTextProcessor`, and emit `tts_ready_chunk` events for completed sentences.",
        "details": "• Add a consumer coroutine `self._tts_stream()` started alongside the UI stream.\n• For each token from the queue:\n   1. Pass token to `TextChunkProcessor.process`.\n   2. If a complete sentence is returned, feed it to `TTSTextProcessor.clean`.\n   3. Send `tts_ready_chunk` with payload `{ \"chunk\": cleaned_sentence }`.\n• On stream termination, call `TextChunkProcessor.flush()`, clean the residual text, and emit a final `tts_ready_chunk` if non‑empty.\n• Ensure ordering: TTS chunks may be slower; they must not block UI token emission – both consumers read from the same queue but process independently (use separate `asyncio.Queue` copies via `asyncio.Queue.get` with `await` and `queue.task_done`).\n• Handle cancellation gracefully (same logic as UI stream).",
        "testStrategy": "Functional test: send a `send_message` with a prompt that yields multiple sentences. Capture `tts_ready_chunk` events and verify they correspond to full sentences, are cleaned (no artifacts), and appear after the appropriate tokens. Also test that a final chunk is emitted after `stream_end` when the last sentence lacks punctuation.",
        "priority": "medium",
        "dependencies": [
          4,
          7,
          5
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Tool Call Transparency Events",
        "description": "Forward LangGraph tool execution events to the client as `tool_call_start` and `tool_call_end` WebSocket messages.",
        "details": "• Extend the producer/consumer pipeline to listen for `on_tool_start` and `on_tool_end` events.\n• When `on_tool_start` is received, extract `tool_name` and `tool_input` and send:\n  `{ \"event\": \"tool_call_start\", \"data\": { \"tool_name\": ..., \"tool_input\": ... } }`.\n• When `on_tool_end` is received, extract `tool_name` and `tool_output` and send `tool_call_end`.\n• Ensure these events are emitted in real‑time, independent of token streams.\n• Update `MessageProcessor.cleanup()` to cancel any pending tool‑related tasks.\n• Add optional logging of tool execution duration for observability.",
        "testStrategy": "Mock a LangGraph that includes a dummy tool node. Verify that the client receives `tool_call_start` immediately before the tool runs and `tool_call_end` after completion, with correct payloads. Also test that interruption during a tool call cancels the tool and still sends `tool_call_end` with an error flag if needed.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Production Enhancements: Persistent Memory, Structured Logging & Metrics",
        "description": "Replace in‑memory `MemorySaver` with `RedisSaver`, add JSON‑structured logging (including `conversation_id` and `turn_id`), and expose Prometheus metrics for key operational indicators.",
        "details": "• Install `redis` and `aioredis` libraries.\n• Update `get_graph` (Task 3) to accept a `checkpointer` argument; default to `RedisSaver` when `REDIS_URL` env var is set.\n• Migrate existing conversation histories to Redis (simple migration script).\n• Integrate `structlog` or Python `logging` with a JSON formatter; inject `conversation_id`, `turn_id`, and `connection_id` into every log record.\n• Add a `/metrics` endpoint using `prometheus_client` exposing gauges/counters:\n   - `websocket_active_connections`\n   - `turn_processing_latency_seconds` (histogram)\n   - `time_to_first_token_seconds` (histogram)\n   - `langgraph_node_errors_total` (counter)\n• Increment/decrement gauges on connection open/close, record latencies around `MessageProcessor.start_turn`.\n• Ensure metrics are thread‑safe and do not block the async event loop.\n• Update Dockerfile / deployment scripts to include Redis connection and expose port 8000 for metrics.",
        "testStrategy": "End‑to‑end test suite:\n  - Spin up a Redis container, run the server, send a conversation, disconnect, reconnect, and verify that prior messages are retrieved from Redis.\n  - Use `curl http://localhost:8000/metrics` to confirm metric values increase appropriately.\n  - Inspect log files to ensure each entry is valid JSON with required fields.\n  - Load‑test with 100 concurrent WebSocket connections and assert no memory leaks and that `websocket_active_connections` reflects the correct count.",
        "priority": "low",
        "dependencies": [
          3,
          1
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-10-23T04:23:05.122Z",
      "updated": "2025-10-23T05:14:11.438Z",
      "description": "Tasks for master context"
    }
  }
}
