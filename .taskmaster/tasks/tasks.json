{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "WebSocket Gateway Setup",
        "description": "Create the asynchronous WebSocket server (FastAPI) with connection lifecycle, authentication (`authorize`), heartbeat (`ping`/`pong`), and basic error handling.",
        "details": "• Define a single endpoint `wss://api.example.com/v1/chat/stream`.\n• On connection, generate a `connection_id` and store auth state in an in‑memory dict.\n• Implement message router that deserializes JSON, validates schema (using pydantic models), and forwards to the appropriate handler.\n• Send `authorize_success` on valid token, close with error on failure.\n• Implement periodic `ping` messages and expect `pong` replies.\n• Ensure graceful shutdown of connections and cleanup of per‑connection resources.",
        "testStrategy": "Use pytest‑asyncio and websockets client to: (a) connect and receive `ping`, reply `pong`; (b) send valid/invalid `authorize` payloads and verify `authorize_success` or error; (c) disconnect and assert server releases resources (no lingering tasks).",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "MessageProcessor Core Orchestrator",
        "description": "Implement the `MessageProcessor` class that supervises a single conversational turn, tracks asyncio.Tasks (supervisor pattern), and provides cleanup and interruption logic.",
        "details": "• Define `MessageProcessor` with methods `start_turn(conversation_id, user_input)`, `handle_interrupt()`, and `cleanup()`.\n• Maintain a set of active tasks (`self.active_tasks: set[asyncio.Task]`) and a bounded per‑turn `asyncio.Queue`.\n• Consume AgentService streaming events (not raw LangGraph callbacks) and forward them into the queue.\n• Wrap turn execution in try/finally and call `handle_interrupt()` in finally to guarantee resource cleanup.\n• Ensure `cleanup()` cancels and awaits all tasks, drains the queue, and logs the turn end.\n• Integrate with the WebSocket router from Task 1: on `send_message` instantiate a new `MessageProcessor` keyed by `conversation_id`; on `interrupt_stream` call `handle_interrupt()`.\n• Use `asyncio.create_task` for each background coroutine and keep references in `self.active_tasks`.",
        "testStrategy": "Unit test `MessageProcessor` by mocking a token producer coroutine: verify tasks are registered, `cleanup()` cancels them, and `handle_interrupt()` stops token emission. Use `asyncio.wait_for` to ensure no dangling tasks after interruption.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "AgentFactory/AgentService streaming integration",
        "description": "Integrate `AgentFactory` + `AgentService` with `MessageProcessor` to provide streaming using the project’s OpenAI‑compatible chat agent and tools.",
        "details": "• Use `src/services/agent_service/agent_factory.py` to construct the agent from YAML configs and create `AgentService` in `src/services/agent_service/service.py`.\n• Ensure the service streams structured events via `service.stream(...)` with types: 'stream_start', 'stream_token', 'tool_call', 'tool_result', 'stream_end', 'error'.\n• Wire the stream to `MessageProcessor`'s queue‑based architecture.\n• Pass conversation scoping via `configurable={\"thread_id\": conversation_id}` or equivalent and, for prototype, use in‑memory `MemorySaver` (no Redis).\n• Update references to use `src/services/agent_service/openai_chat_agent.py` rather than non‑existent `agents/chat_agent.py`.",
        "testStrategy": "Integration test that: (1) Creates ChatAgent via factory, (2) Sends test message through MessageProcessor with ChatAgent integration, (3) Verifies streaming responses contain proper content chunks and tool events, (4) Confirms memory persistence across conversation turns, (5) Tests interruption handling during streaming.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Producer‑Consumer Pipeline with backpressure",
        "description": "Implement the async producer that consumes AgentService streaming events and feeds a bounded `asyncio.Queue`. Only `stream_start`, `tts_ready_chunk`, and `stream_end` leave the server, while internal `stream_token` flow continues uninterrupted toward the TTS pipeline.",
        "details": "• In `MessageProcessor.start_conversation_turn`, create `self._event_queue = asyncio.Queue(maxsize=100)` for streaming events.\n• Launch a producer task: `asyncio.create_task(self._produce_agent_events(agent_service, messages, turn_id))` where `_produce_agent_events` iterates over `agent_service.stream()` and `await self._event_queue.put(event)`.\n• The AgentService yields: 'stream_start', 'stream_token', 'tool_call', 'tool_result', 'stream_end', 'error'.\n• Producer must handle `asyncio.CancelledError` and cleanup gracefully on interruption.\n• Keep enqueuing 'stream_token' events immediately for downstream processing even while the consumer is emitting TTS chunks; rely on the bounded queue to provide backpressure instead of pausing agent generation.\n• Do NOT forward 'stream_token' or any tool events directly to clients. They are inputs to the TTS pipeline or logs.\n• Client‑facing mapping: 'stream_start' → STREAM_START; completed TTS sentences → TTS_READY_CHUNK; 'stream_end' → STREAM_END; 'error' → ERROR.\n• Log queue size periodically for observability and ensure backpressure prevents memory issues.",
        "testStrategy": "Mock `AgentService.stream()` to emit rapid events (>200/sec) with different types. Verify producer correctly maps event types to WebSocket message formats. Test that queue maxsize prevents memory overflow and that `asyncio.CancelledError` during agent streaming is handled cleanly. Verify that the existing `MessageProcessor.interrupt_turn()` correctly cancels the producer task.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Client text emission policy — no `stream_token`",
        "description": "Enforce the client contract where only `tts_ready_chunk` (cleaned full sentences) and lifecycle events are emitted while token flow remains internal.",
        "details": "• Do not create a separate UI token consumer.\n• Ensure no `stream_token` messages are sent to clients; instead, tokens feed the TTS pipeline.\n• Keep a single turn lifecycle visible to clients via `stream_start` and `stream_end`.\n• Update the WebSocket protocol docs/PRD, API reference, and client-facing release notes to reflect the new contract and adjust automated integration tests accordingly.",
        "testStrategy": "Functional test: send `send_message`, assert the client receives `stream_start`, then only `tts_ready_chunk` events (one per sentence), and finally `stream_end`. Assert zero `stream_token` messages are received.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Interrupt Stream Handling",
        "description": "Allow the client to cancel an active turn via `interrupt_stream` and ensure all related tasks stop safely.",
        "details": "• In the WebSocket router, map `interrupt_stream` to `MessageProcessor.handle_interrupt()`.\n• `handle_interrupt` must:\n   1. Cancel the producer task and all consumer tasks (`self._tasks.values()`).\n   2. Drain the queue (`while not self._queue.empty(): self._queue.get_nowait()`).\n   3. Send a `stream_end` event with the current `turn_id` to guarantee client state consistency.\n   4. Call `cleanup()` to release resources.\n• Guard against race conditions where a token may be in flight; use `asyncio.wait` with timeout to ensure cancellation completes.\n• Log interruption reason for observability.",
        "testStrategy": "Integration test: start a long‑running turn (e.g., a prompt that yields many tokens), send `interrupt_stream` after a few tokens, and assert that no further `stream_token` or `tts_ready_chunk` events are received. Verify that a single `stream_end` is still emitted.",
        "priority": "high",
        "dependencies": [
          2,
          4,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "TextChunkProcessor (generator) & TTSTextProcessor (configurable rules)",
        "description": "Create the two‑stage pipeline that buffers raw tokens, yields complete sentences, and cleans them for TTS using externally configurable regex rules.",
        "details": "• `TextChunkProcessor`\n   - Internal buffer `self._buf` (string).\n   - `process(token: str) -> Generator[str, None, None]`: append token; find the last sentence terminator among `.?!` and yield one or more completed sentences; keep remainder in buffer.\n   - `flush() -> Optional[str]`: return any remaining buffered text (or None) at stream end.\n• `TTSTextProcessor`\n   - Stateless class with method `process(text: str) -> str` applying regex rules loaded from a YAML/JSON config file (e.g., at `yaml_files/tts_rules.yml`), with a sensible default when file missing.\n   - Support replacements like removing fillers `(웃음)`, `음...`, and expanding abbreviations.\n• Unit tests for: multiple terminators per batch, no terminator then `flush()`, multilingual punctuation, and cleaning rules application.\n• Ensure new processor instances are created on `stream_start` and `flush()` is invoked on `stream_end` to avoid cross‑source mixing.",
        "testStrategy": "Unit tests covering:\n  - Generator yielding multiple sentences from a single token batch.\n  - `flush()` behavior when no terminator.\n  - Rules file loading (present/missing) and regex application.\n  - Integration test piping a token stream through both processors and verifying `tts_ready_chunk` outputs.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "TTS Stream — sentence chunk emission (single consumer)",
        "description": "Consume token events, run them through `TextChunkProcessor` and `TTSTextProcessor`, and emit `tts_ready_chunk` events for completed sentences (no parallel UI token consumer).",
        "details": "• Add a consumer coroutine `self._tts_stream()` as the only client‑facing text emitter.\n• For each 'stream_token' event from the queue:\n   1. Pass token to `TextChunkProcessor.process` (generator).\n   2. For each yielded sentence, feed it to `TTSTextProcessor.process`.\n   3. Send `tts_ready_chunk` with payload `{ \"chunk\": cleaned_sentence }`.\n• On stream termination, call `TextChunkProcessor.flush()`, clean the residual text, and emit a final `tts_ready_chunk` if non‑empty.\n• Handle cancellation gracefully and ensure `stream_end` is emitted once per turn.",
        "testStrategy": "Functional test: send a `send_message` with a prompt that yields multiple sentences. Capture `tts_ready_chunk` events and verify they correspond to full sentences and are cleaned. Also test that a final chunk is emitted after `stream_end` when the last sentence lacks punctuation.",
        "priority": "medium",
        "dependencies": [
          4,
          7
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Tool events — backend logging only",
        "description": "Do not emit tool events to clients. Log `tool_call` and `tool_result` server‑side with structured metadata.",
        "details": "• Extend the producer/consumer pipeline to listen for tool events but do not forward them over WebSocket.\n• Log fields: `conversation_id`, `turn_id`, `tool_name`, `args`, `duration_ms`, `status` (success/error).\n• Ensure logs are JSON‑structured and correlate with other turn logs.\n• Confirm that omission of client tool events is documented in API docs.",
        "testStrategy": "Mock tool events and verify: (a) no tool messages are sent to client, (b) JSON logs are produced with required fields, (c) durations are captured.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Production Enhancements: Persistent Memory, Structured Logging & Metrics",
        "description": "Replace in‑memory `MemorySaver` with `RedisSaver`, add JSON‑structured logging (including `conversation_id` and `turn_id`), and expose Prometheus metrics for key operational indicators.",
        "details": "• Install `redis` and `aioredis` libraries.\n• Update `get_graph` (Task 3) to accept a `checkpointer` argument; default to `RedisSaver` when `REDIS_URL` env var is set.\n• Migrate existing conversation histories to Redis (simple migration script).\n• Integrate `structlog` or Python `logging` with a JSON formatter; inject `conversation_id`, `turn_id`, and `connection_id` into every log record.\n• Add a `/metrics` endpoint using `prometheus_client` exposing gauges/counters:\n   - `websocket_active_connections`\n   - `turn_processing_latency_seconds` (histogram)\n   - `time_to_first_token_seconds` (histogram)\n   - `langgraph_node_errors_total` (counter)\n• Increment/decrement gauges on connection open/close, record latencies around `MessageProcessor.start_turn`.\n• Ensure metrics are thread‑safe and do not block the async event loop.\n• Update Dockerfile / deployment scripts to include Redis connection and expose port 8000 for metrics.",
        "testStrategy": "End‑to‑end test suite:\n  - Spin up a Redis container, run the server, send a conversation, disconnect, reconnect, and verify that prior messages are retrieved from Redis.\n  - Use `curl http://localhost:8000/metrics` to confirm metric values increase appropriately.\n  - Inspect log files to ensure each entry is valid JSON with required fields.\n  - Load‑test with 100 concurrent WebSocket connections and assert no memory leaks and that `websocket_active_connections` reflects the correct count.",
        "priority": "low",
        "dependencies": [
          3,
          1
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-10-23T04:23:05.122Z",
      "updated": "2025-10-25T15:14:11.267Z",
      "description": "Tasks for master context"
    }
  }
}
